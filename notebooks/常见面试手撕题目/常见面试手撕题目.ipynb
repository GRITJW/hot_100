{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60cf0595",
   "metadata": {},
   "source": [
    "## 1.注意力机制\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87909a4",
   "metadata": {},
   "source": [
    "### MHA\n",
    "\n",
    "multiheadattention(torch.nn.Module):\n",
    "\n",
    "这表示定义一个名为 multiheadattention 的类，并且它继承自 nn.Module。nn.Module 是 PyTorch 中所有神经网络模块的基类，提供参数管理、子模块注册、前向传播接口、模型保存加载、.to(device)、.eval() 等核心功能。继承它的本质含义是：让 multiheadattention 成为一个“可训练的神经网络模块”，内部定义的层（如 nn.Linear）会自动被注册为可学习参数，从而能被优化器管理并参与反向传播。没有继承 nn.Module，这个类就只是普通 Python 类，不具备深度学习框架的功能。\n",
    "\n",
    "super().init():\n",
    "\n",
    "这行代码调用父类 nn.Module 的构造函数。作用是完成基类的初始化，包括建立参数容器、子模块字典、缓冲区结构等内部机制。如果不调用它，虽然类语法上可以运行，但模块中的参数不会被正确注册，model.parameters() 可能为空，优化器无法更新参数，模型状态也无法正确保存。简言之，它确保当前类真正成为一个“完整初始化的 PyTorch 模块”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ac455cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代码运行成功！\n",
      "输出张量的形状: torch.Size([5, 10, 64])\n",
      "注意力权重的形状: torch.Size([5, 4, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math \n",
    "class multiheadattention(torch.nn.Module):\n",
    "    def __init__(self, d, h):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        self.k = d // h\n",
    "\n",
    "        self.wqkv = torch.nn.Linear(d, d*3)\n",
    "        self.wo = torch.nn.Linear(d, d)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        B, L, D = x.shape\n",
    "        qkv = self.wqkv(x)\n",
    "        q, k, v = torch.chunk(qkv, 3, -1)\n",
    "        # 可以用reshap代替transpose\n",
    "        q = q.view(B, L, self.h, self.k).transpose(1, 2)\n",
    "        k = k.view(B, L, self.h, self.k).transpose(1, 2)\n",
    "        v = v.view(B, L, self.h, self.k).transpose(1, 2)\n",
    "\n",
    "        attention_score =torch.matmul(q, k.transpose(-1,-2)) / math.sqrt(self.k)\n",
    "        if mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask=mask, value=-1e9)\n",
    "        attention_weight = torch.softmax(attention_score, dim=-1)\n",
    "        context = torch.matmul(attention_weight, v).transpose(1, 2).contiguous().view(B, L, D)\n",
    "        output = self.wo(context)\n",
    "        return output, attention_weight\n",
    "    \n",
    "# --- 测试代码 ---\n",
    "batch_size = 5\n",
    "max_seq_len = 10\n",
    "d_model = 64\n",
    "head = 4\n",
    "\n",
    "x = torch.randn(batch_size, max_seq_len, d_model)\n",
    "\n",
    "attention_model = multiheadattention(d_model, head)\n",
    "output, attention = attention_model(x) \n",
    "\n",
    "print(\"代码运行成功！\")\n",
    "print(\"输出张量的形状:\", output.shape)\n",
    "print(\"注意力权重的形状:\", attention.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b6a2a",
   "metadata": {},
   "source": [
    "### MQA\n",
    "\n",
    "多查询注意力：所有查询头共用单一的键值头\n",
    "\n",
    "与原 MultiHeadAttention 的差异：\n",
    "- Q: 维持与 MHA 相同，shape -> [B, h, L, d_k]\n",
    "- K/V: 仅产生 1 组共享头，shape -> [B, 1, L, d_k]\n",
    "这样在推理时 KV cache 只需缓存 1 份（而非 h 份）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c42a71c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代码运行成功！（MQA）\n",
      "输出张量的形状: torch.Size([2, 5, 10])\n",
      "注意力权重的形状: torch.Size([2, 2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_head):\n",
    "        super(MultiQueryAttention, self).__init__()\n",
    "        assert d_model % num_head == 0, \"d_model 必须能被 num_head 整除\"\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.d_k = d_model // num_head\n",
    "\n",
    "        # 与原实现的区别：\n",
    "        #   - Q 仍然映射到 d_model（然后 reshape 成 h 个头）\n",
    "        #   - K/V 只映射到 d_k（单头维度），且各 1 份\n",
    "        self.wq = nn.Linear(d_model, d_model)      # 生成多头的 Q\n",
    "        self.wkv = nn.Linear(d_model, 2 * self.d_k)  # 生成共享的 K、V（仅 1 头的维度）\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        # 1) 计算 Q、K、V\n",
    "        q = self.wq(x)                       # [B, L, d_model]\n",
    "        kv = self.wkv(x)                     # [B, L, 2*d_k]\n",
    "        k, v = torch.chunk(kv, 2, dim=-1)    # [B, L, d_k], [B, L, d_k]\n",
    "\n",
    "        # 2) 形状整理\n",
    "        # Q: [B, L, h, d_k] -> [B, h, L, d_k]\n",
    "        q = q.view(B, L, self.num_head, self.d_k).transpose(1, 2)  # [B, h, L, d_k]\n",
    "\n",
    "        # 共享 K/V：加一个“伪 head 维”=1，方便广播到 h\n",
    "        # K/V: [B, L, d_k] -> [B, 1, L, d_k]，unsqueeze(dim) = 在 dim 这个位置插入一个长度为1的轴\n",
    "        k = k.unsqueeze(1)  # [B, 1, L, d_k]\n",
    "        v = v.unsqueeze(1)  # [B, 1, L, d_k]\n",
    "\n",
    "        # 3) 注意力分数：Q 与共享 K\n",
    "        # scores: [B, h, L, L]，这里利用了 K 在 head 维度上的广播\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.d_k)  # [B, h, L, L]\n",
    "\n",
    "        if mask is not None:\n",
    "            # 要求 mask 能广播到 [B, h, L, L]\n",
    "            # 例如 mask 形状可为 [B, 1, 1, L]（causal/ padding），或 [B, 1, L, L]\n",
    "            scores = scores.masked_fill(mask, -1e9)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)         # [B, h, L, L]\n",
    "\n",
    "        # 4) 加权求和：与共享 V 相乘（同样通过广播）\n",
    "        context = torch.matmul(attn, v)              # [B, h, L, d_k]\n",
    "\n",
    "        # 5) 还原回 [B, L, d_model] 并输出\n",
    "        context = context.transpose(1, 2).contiguous()         # [B, L, h, d_k]\n",
    "        context = context.view(B, L, self.d_model)             # [B, L, d_model]\n",
    "        output = self.wo(context)                               # [B, L, d_model]\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "# --- 简单测试（与原 MHA 测试保持一致） ---\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 2\n",
    "    d_model = 10\n",
    "    head = 2\n",
    "    max_seq_len = 5\n",
    "    x = torch.randn(batch_size, max_seq_len, d_model)\n",
    "\n",
    "    attention_model = MultiQueryAttention(d_model, head)\n",
    "    output, attention = attention_model(x)\n",
    "\n",
    "    print(\"代码运行成功！（MQA）\")\n",
    "    print(\"输出张量的形状:\", output.shape)     # 期望: [B, L, d_model]\n",
    "    print(\"注意力权重的形状:\", attention.shape)  # 期望: [B, h, L, L]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ea8c8",
   "metadata": {},
   "source": [
    "### GQA\n",
    "\n",
    "分组查询注意力：采用折中策略，将h个查询头分为g组\n",
    "\n",
    "GQA: 把 h 个 Query 头分成 g 组；组内共享 1 套 K/V\n",
    "- num_head = h\n",
    "- num_kv_head = g (1 < g <= h, 且 h % g == 0)\n",
    "- \n",
    "形状约定：\n",
    "    Q: [B, h, L, d_k]\n",
    "    K,V(分组): [B, g, L, d_k]\n",
    "\n",
    "计算时把 Q reshape 成 [B, g, h_per_group, L, d_k]，\n",
    "\n",
    "与同组的 K,V 做注意力；最后再还原回 [B, h, L, d_k]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9fb0aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代码运行成功！（GQA）\n",
      "输出张量形状: torch.Size([2, 5, 12])\n",
      "注意力形状  : torch.Size([2, 6, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_head, num_kv_head):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        assert d_model % num_head == 0, \"d_model 必须能被 num_head 整除\"\n",
    "        assert 1 <= num_kv_head <= num_head, \"num_kv_head 必须在 [1, num_head] 范围内\"\n",
    "        assert num_head % num_kv_head == 0, \"num_head 必须能被 num_kv_head 整除（每组等量分配）\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head            # h\n",
    "        self.num_kv_head = num_kv_head      # g\n",
    "        self.d_k = d_model // num_head\n",
    "        self.h_per_group = self.num_head // self.num_kv_head  # h/g\n",
    "\n",
    "        # Q 仍映射到 d_model（随后 reshape 为 h 个头）\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # K/V 只映射到 g * d_k（随后 reshape 为 g 个“KV 头”）\n",
    "        self.wkv = nn.Linear(d_model, 2 * self.num_kv_head * self.d_k)\n",
    "\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        # 1) 投影\n",
    "        # Q: [B, L, d_model]\n",
    "        # KV: [B, L, 2 * g * d_k] -> split -> [B, L, g * d_k] 各自\n",
    "        q = self.wq(x)\n",
    "        kv = self.wkv(x)\n",
    "        k, v = torch.chunk(kv, 2, dim=-1)\n",
    "\n",
    "        # 2) 形状整理\n",
    "        # Q -> [B, h, L, d_k]\n",
    "        q = q.view(B, L, self.num_head, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # K,V -> [B, g, L, d_k]\n",
    "        k = k.view(B, L, self.num_kv_head, self.d_k).transpose(1, 2)\n",
    "        v = v.view(B, L, self.num_kv_head, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 把 Q 分组： [B, h, L, d_k] -> [B, g, h_per_group, L, d_k]\n",
    "        qg = q.view(B, self.num_kv_head, self.h_per_group, L, self.d_k)\n",
    "\n",
    "        # 为了与组内 K,V 做 batched matmul，给 K,V 加一个组内头维度=1，方便广播\n",
    "        # Kg, Vg: [B, g, 1, L, d_k]\n",
    "        Kg = k.unsqueeze(2)\n",
    "        Vg = v.unsqueeze(2)\n",
    "\n",
    "        # 3) 组内注意力分数： [B, g, h_per_group, L, L]\n",
    "        # 等价于：scores_g[b,g,hg] = qg[b,g,hg] @ Kg[b,g,0]^T / sqrt(d_k)\n",
    "        scores_g = torch.matmul(qg, Kg.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            # 要求 mask 能广播到 [B, 1 或 g, 1 或 h_per_group, L, L] 或最终 [B, h, L, L]\n",
    "            # 最常见做法：提供 [B, 1, 1, L, L]（或 [B, 1, 1, 1, L] 的causal/pad组合）\n",
    "            scores_g = scores_g.masked_fill(mask, -1e9)\n",
    "\n",
    "        attn_g = torch.softmax(scores_g, dim=-1)               # [B, g, h_per_group, L, L]\n",
    "\n",
    "        # 4) 组内加权求和：context_g: [B, g, h_per_group, L, d_k]\n",
    "        context_g = torch.matmul(attn_g, Vg)\n",
    "\n",
    "        # 5) 还原回所有头：先合并 g 与 h_per_group -> h\n",
    "        context = context_g.reshape(B, self.num_head, L, self.d_k).transpose(1, 2).reshape(B, L, self.d_model)\n",
    "        # context = context_g.reshape(B, self.num_head, L, self.d_k)  # [B, h, L, d_k]\n",
    "        # context = context.transpose(1, 2).contiguous()              # [B, L, h, d_k]\n",
    "        # context = context.view(B, L, self.d_model)                  # [B, L, d_model]\n",
    "        output = self.wo(context)\n",
    "\n",
    "        # 同样给出注意力权重（按头展平回 [B, h, L, L]，便于对齐可视化）\n",
    "        attn = attn_g.reshape(B, self.num_head, L, L)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "# --- 简单测试（与原 MHA 测试风格一致） ---\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 2\n",
    "    d_model = 12\n",
    "    num_head = 6     # h\n",
    "    num_kv_head = 3  # g（每组 2 个 Q 头）\n",
    "    max_seq_len = 5\n",
    "\n",
    "    x = torch.randn(batch_size, max_seq_len, d_model)\n",
    "    gqa = GroupedQueryAttention(d_model, num_head, num_kv_head)\n",
    "    out, attn = gqa(x)\n",
    "\n",
    "    print(\"代码运行成功！（GQA）\")\n",
    "    print(\"输出张量形状:\", out.shape)      # 期望: [B, L, d_model]\n",
    "    print(\"注意力形状  :\", attn.shape)     # 期望: [B, h, L, L]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d544f",
   "metadata": {},
   "source": [
    "练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b6ed202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 25, 64])\n",
      "torch.Size([10, 4, 2, 25, 25])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class GQA(nn.Module):\n",
    "    def __init__(self, dim, q_head, kv_head):\n",
    "        super().__init__()\n",
    "        self.d = dim\n",
    "        self.q_head = q_head\n",
    "        self.kv_head = kv_head\n",
    "        self.d_k = dim // q_head\n",
    "        self.h_per_kv = q_head // kv_head\n",
    "\n",
    "        self.wq = nn.Linear(dim, dim)\n",
    "        self.wkv = nn.Linear(dim, 2*kv_head*self.d_k)\n",
    "        self.wo = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        B, L, D = x.shape\n",
    "        q = self.wq(x)\n",
    "        kv = self.wkv(x)\n",
    "        k, v = torch.chunk(kv, 2, -1)\n",
    "        q = q.view(B, L, self.q_head, self.d_k).transpose(1, 2)\n",
    "        k = k.view(B, L, self.kv_head, self.d_k).transpose(1, 2)\n",
    "        v = v.view(B, L, self.kv_head, self.d_k).transpose(1, 2)\n",
    "        qg = q.view(B, self.kv_head, self.h_per_kv, L, self.d_k)\n",
    "        kg = k.unsqueeze(2)\n",
    "        vg = v.unsqueeze(2)\n",
    "        attention_scores = torch.matmul(qg, kg.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask, -1e9)\n",
    "        attention_weight = nn.functional.softmax(attention_scores, dim = -1)\n",
    "\n",
    "        context = torch.matmul(attention_weight, vg).reshape(B, self.q_head, L, self.d_k).transpose(1, 2).reshape(B, L, self.d)\n",
    "        output = self.wo(context)\n",
    "        return output, attention_weight\n",
    "\n",
    "B = 10\n",
    "L = 25\n",
    "D = 64\n",
    "q_head = 8\n",
    "kv_head = 4\n",
    "x = torch.randn(B, L, D)\n",
    "GQA_model = GQA(D, q_head, kv_head)\n",
    "output, attn = GQA_model(x)\n",
    "print(output.shape)\n",
    "print(attn.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18dcedd",
   "metadata": {},
   "source": [
    "## 2.AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f4d59",
   "metadata": {},
   "source": [
    "### AUC\n",
    "基于排序（Rank / Mann–Whitney U）的 AUC 计算方法。\n",
    "\n",
    "输入：\n",
    "- labels: List[int] 或 1D numpy array\n",
    "    样本真实标签，取值为 {0, 1}\n",
    "- scores: List[float] 或 1D numpy array\n",
    "    模型预测分数，分数越大表示越可能为正样本\n",
    "\n",
    "输出：\n",
    "- auc: float\n",
    "    AUC 值，取值范围 [0, 1]\n",
    "\n",
    "核心思想：对标签进行排序\n",
    "1. 按预测分数从小到大排序\n",
    "2. 扫描排序后的样本序列\n",
    "3. 每遇到一个正样本，统计其前面已有多少负样本\n",
    "    这些负样本都被该正样本“正确地排在后面”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cdd950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:[0 1 0 0 1 0 0 1]\n",
      "scores:[0.1 0.9 0.2 0.8 1.  0.2 0.3 0.8]\n",
      "order:[0 2 5 6 3 7 1 4]\n",
      "labels_sorted:[0 0 0 0 0 1 1 1]\n",
      "n_pos:3\n",
      "n_neg:5\n",
      "auc:1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "def auc_rank(labels, scores):\n",
    " \n",
    "    # 转为 numpy array，便于排序和向量化操作\n",
    "    labels = np.asarray(labels)\n",
    "    scores = np.asarray(scores)\n",
    "    print(f\"labels:{labels}\")\n",
    "    print(f\"scores:{scores}\")\n",
    "\n",
    " \n",
    "    # 获取按照 score 从小到大排序后的索引\n",
    "    order = np.argsort(scores)\n",
    "    print(f\"order:{order}\")\n",
    " \n",
    "    # 按排序后的顺序重排标签\n",
    "    labels_sorted = labels[order]\n",
    "    print(f\"labels_sorted:{labels_sorted}\")\n",
    " \n",
    " \n",
    "    # 正样本数量 |P|\n",
    "    n_pos = np.sum(labels_sorted == 1)\n",
    "    print(f\"n_pos:{n_pos}\")\n",
    " \n",
    "    # 负样本数量 |N|\n",
    "    n_neg = np.sum(labels_sorted == 0)\n",
    "    print(f\"n_neg:{n_neg}\")\n",
    "    # 已扫描到的负样本数量（前缀负样本计数）\n",
    "    neg_count = 0\n",
    " \n",
    "    # 排序正确的正负样本对数量\n",
    "    correct = 0.0\n",
    " \n",
    "    # 从低分到高分扫描\n",
    "    for l in labels_sorted:\n",
    "        if l == 1:\n",
    "            # 当前是正样本：\n",
    "            # 它前面的所有负样本都满足 score_neg < score_pos\n",
    "            correct += neg_count\n",
    "        else:\n",
    "            # 当前是负样本，增加负样本计数\n",
    "            neg_count += 1\n",
    " \n",
    "    # AUC = 排序正确的正负样本对 / 总正负样本对\n",
    "    return correct / (n_pos * n_neg)\n",
    "\n",
    "label = [0, 1, 0, 0, 1, 0, 0, 1]\n",
    "q = [0.1, 0.9, 0.2, 0.8, 1, 0.2, 0.3, 0.8]\n",
    "auc  = auc_rank(label, q)\n",
    "print(f\"auc:{auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed20f7",
   "metadata": {},
   "source": [
    "练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0b0511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def auc_rank(labels, scores):\n",
    "    labels = np.array(labels)\n",
    "    scores = np.array(scores)\n",
    "    order = np.argsort(scores)\n",
    "    labels_ordered = labels[order]\n",
    "    pos_sample = np.sum(labels_ordered == 1)\n",
    "    neg_sample = np.sum(labels_ordered == 0)\n",
    "    negative = 0\n",
    "    positive = 0\n",
    "    for i in labels_ordered:\n",
    "        if i == 0:\n",
    "            negative += 1\n",
    "        else:\n",
    "            positive += negative\n",
    "    auc = positive / (pos_sample * neg_sample)\n",
    "    return auc\n",
    "label = [0, 1, 0, 0, 1, 0, 0, 1]\n",
    "q = [0.1, 0.9, 0.2, 0.8, 1, 0.2, 0.3, 0.8]\n",
    "auc  = auc_rank(label, q)\n",
    "print(f\"auc:{auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6e291",
   "metadata": {},
   "source": [
    "### GAUC\n",
    "基于排序的 GAUC 计算方法(Group AUC)。\n",
    "\n",
    "GAUC 通过在每个用户内部分别计算 AUC,然后进行加权平均,\n",
    "从而消除用户间基线差异的影响,更准确地评估模型的用户内排序能力。\n",
    "\n",
    "输入:\n",
    "- user_ids: List[str] 或 1D numpy array\n",
    "    每个样本对应的用户标识\n",
    "- labels: List[int] 或 1D numpy array  \n",
    "    样本真实标签,取值为 {0, 1}\n",
    "- scores: List[float] 或 1D numpy array\n",
    "    模型预测分数,分数越大表示越可能为正样本\n",
    "- weight_type: str, 默认 'impression'\n",
    "    权重类型,可选值:\n",
    "    - 'impression': 按用户曝光次数加权(工业界常用)\n",
    "    - 'uniform': 等权重,每个用户权重为 1\n",
    "    \n",
    "输出:\n",
    "- gauc: float\n",
    "    加权后的 GAUC 值,取值范围 [0, 1]\n",
    "- user_auc_dict: dict\n",
    "    每个用户的 AUC 值字典,用于分析不同用户的排序质量\n",
    "    \n",
    "核心思想:\n",
    "1. 将样本按 user_id 分组\n",
    "2. 在每个用户内部,使用 Rank-based 方法计算 AUC\n",
    "3. 根据指定的权重类型对所有用户的 AUC 进行加权平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d30a6af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gauc:0.8636363636363636\n",
      "user_auc_dict:{1: 1.0, 2: 1.0, 3: 0.5}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    " \n",
    "def gauc_rank(user_ids, labels, scores, weight_type='impression'):\n",
    "    \n",
    "    # 转为 numpy array\n",
    "    user_ids = np.asarray(user_ids)\n",
    "    labels = np.asarray(labels)\n",
    "    scores = np.asarray(scores)\n",
    "    \n",
    "    # 按用户分组:构建 user_id -> 样本索引列表 的映射\n",
    "    user_sample_dict = defaultdict(list)\n",
    "    for idx, user_id in enumerate(user_ids):\n",
    "        user_sample_dict[user_id].append(idx)\n",
    "    \n",
    "    # 存储每个用户的 AUC 和权重\n",
    "    user_auc_dict = {}\n",
    "    total_weighted_auc = 0.0\n",
    "    total_weight = 0.0\n",
    "    \n",
    "    # 遍历每个用户,计算其 AUC\n",
    "    for user_id, sample_indices in user_sample_dict.items():\n",
    "        # 提取该用户的所有样本,user_id=1,sample_indices= [0, 1, 2, 3]\n",
    "        user_labels = labels[sample_indices]\n",
    "        user_scores = scores[sample_indices]\n",
    "        \n",
    "        # 计算该用户内的正负样本数\n",
    "        n_pos = np.sum(user_labels == 1)\n",
    "        n_neg = np.sum(user_labels == 0)\n",
    "        \n",
    "        # 如果该用户只有正样本或只有负样本,无法计算 AUC\n",
    "        # 跳过该用户(也可以选择赋值为 0.5)\n",
    "        if n_pos == 0 or n_neg == 0:\n",
    "            continue\n",
    "            \n",
    "        # 使用 Rank-based 方法计算该用户的 AUC\n",
    "        # 按 score 从小到大排序\n",
    "        order = np.argsort(user_scores)\n",
    "        sorted_labels = user_labels[order]\n",
    "        \n",
    "        # 统计排序正确的正负样本对数\n",
    "        neg_count = 0\n",
    "        correct_pairs = 0.0\n",
    "        \n",
    "        for label in sorted_labels:\n",
    "            if label == 1:\n",
    "                # 正样本:其前面的所有负样本都被正确排序\n",
    "                correct_pairs += neg_count\n",
    "            else:\n",
    "                # 负样本:计数增加\n",
    "                neg_count += 1\n",
    "        \n",
    "        # 该用户的 AUC\n",
    "        user_auc = correct_pairs / (n_pos * n_neg)\n",
    "        user_auc_dict[user_id] = user_auc\n",
    "        \n",
    "        # 确定该用户的权重\n",
    "        if weight_type == 'impression':\n",
    "            # 按曝光次数加权\n",
    "            weight = len(sample_indices)\n",
    "        elif weight_type == 'uniform':\n",
    "            # 等权重\n",
    "            weight = 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown weight_type: {weight_type}\")\n",
    "        \n",
    "        # 累加加权 AUC\n",
    "        total_weighted_auc += user_auc * weight\n",
    "        total_weight += weight\n",
    "    \n",
    "    # 计算 GAUC\n",
    "    if total_weight == 0:\n",
    "        return 0.5, user_auc_dict\n",
    "    \n",
    "    gauc = total_weighted_auc / total_weight\n",
    "    print(f\"gauc:{gauc}\")\n",
    "    print(f\"user_auc_dict:{user_auc_dict}\")\n",
    "    return gauc, user_auc_dict\n",
    "\n",
    "label = [0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0]\n",
    "q = [0.1, 0.9, 0.2, 0.8, 1, 0.2, 0.3, 0.9, 0.7, 0.9, 0.7]\n",
    "user_id = [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3]\n",
    "_, _ = gauc_rank(user_id, label, q, weight_type='impression')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0e056",
   "metadata": {},
   "source": [
    "练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d40bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    " \n",
    "def gauc_rank(user_ids, labels, scores, weight_type='impression'):\n",
    "    \n",
    "    user_ids = np.asarray(user_ids)\n",
    "    labels = np.asarray(labels)\n",
    "    scores = np.asarray(scores)\n",
    "    user_sample_dict = defaultdict(list)\n",
    "    for idx, user_id in enumerate(user_ids):\n",
    "        user_sample_dict[user_id].append(idx)\n",
    "    user_auc_dict = {}\n",
    "    total_weighted_auc = 0.0\n",
    "    total_weight = 0.0\n",
    "    for user_id, sample_indices in user_sample_dict.items():\n",
    "        user_labels = labels[sample_indices]\n",
    "        user_scores = scores[sample_indices]\n",
    "        n_pos = np.sum(user_labels == 1)\n",
    "        n_neg = np.sum(user_labels == 0)\n",
    "        if n_pos == 0 or n_neg == 0:\n",
    "            continue\n",
    "        order = np.argsort(user_scores)\n",
    "        sorted_labels = user_labels[order]\n",
    "        neg_count = 0\n",
    "        correct_pairs = 0.0\n",
    "        \n",
    "        for label in sorted_labels:\n",
    "            if label == 1:\n",
    "                correct_pairs += neg_count\n",
    "            else:\n",
    "                neg_count += 1\n",
    "        user_auc = correct_pairs / (n_pos * n_neg)\n",
    "        user_auc_dict[user_id] = user_auc\n",
    "        if weight_type == 'impression':\n",
    "            weight = len(sample_indices)\n",
    "        elif weight_type == 'uniform':\n",
    "            weight = 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown weight_type: {weight_type}\")\n",
    "        total_weighted_auc += user_auc * weight\n",
    "        total_weight += weight\n",
    "    if total_weight == 0:\n",
    "        return 0.5, user_auc_dict\n",
    "    gauc = total_weighted_auc / total_weight\n",
    "    print(f\"gauc:{gauc}\")\n",
    "    print(f\"user_auc_dict:{user_auc_dict}\")\n",
    "    return gauc, user_auc_dict\n",
    "\n",
    "label = [0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0]\n",
    "q = [0.1, 0.9, 0.2, 0.8, 1, 0.2, 0.3, 0.9, 0.7, 0.9, 0.7]\n",
    "user_id = [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3]\n",
    "_, _ = gauc_rank(user_id, label, q, weight_type='impression')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a88cc9",
   "metadata": {},
   "source": [
    "## 3.损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ef5df",
   "metadata": {},
   "source": [
    "### BCE二类交叉熵实现\n",
    "\n",
    "1\\. 基础 BCE 公式\n",
    "\n",
    "BCE 的标准形式是基于概率定义的。假设 $y$ 为真实标签 (0 或 1)，$p$ 为模型预测为 1 的概率（即 Sigmoid 的输出），则损失 $L$ 为：\n",
    "\n",
    "$$L = -[y \\cdot \\log(p) + (1-y) \\cdot \\log(1-p)]$$\n",
    "\n",
    "这个公式非常直观，但存在一个致命缺陷：当 $p$ 趋近于 0 或 1 时，$\\\\log(p)$ 或 $\\\\log(1-p)$ 会导致 `log(0)`，产生负无穷大的结果，这在计算上是灾难性的。\n",
    "\n",
    "2\\. 数值稳定的 BCE 公式\n",
    "\n",
    "为了解决这个问题，我们不应该使用模型输出的概率 $p$，而应直接使用未经 Sigmoid 激活的原始输出——**Logits**，我们记为 $x$。\n",
    "\n",
    "通过将 $p = \\\\text{sigmoid}(x)$ 代入原始公式并进行数学推导与化简，我们可以得到一个等价且高度数值稳定的形式：\n",
    "\n",
    "$$L = \\max(x, 0) - x \\cdot y + \\log(1 + e^{-|x|})$$\n",
    "\n",
    "这个公式的核心优势在于，指数项中的 $-|x|$ 永远是非正数，这使得 $e^{-|x|}$ 的结果被限制在 $(0, 1]$ 区间内，从根本上避免了浮点数上溢的风险。它不依赖任何 `epsilon` 裁剪之类的技巧，是数值计算上的最优解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bae18f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bce损失为0.008514077508444018\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def BCE_logits_Loss(lables, y_predict):\n",
    "    per_sample_loss = np.maximum(y_predict, 0) - y_predict * lables + np.log(1 + np.exp(-np.abs(y_predict)))\n",
    "    return np.mean(per_sample_loss)\n",
    "\n",
    "x = np.array([5,-4,5,-6])\n",
    "y = np.array([1,0,1,0])\n",
    "\n",
    "bce_loss = BCE_logits_Loss(y,x)\n",
    "\n",
    "print(f\"bce损失为{bce_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2c53b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bce损失为0.008514077508444016\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def BCE_logits_Loss(lables, y_predict):\n",
    "    p = 1 / (1 + np.exp(-y_predict))\n",
    "    per_sample_loss = -(lables * np.log(p) + (1 - lables) * np.log(1 - p))\n",
    "    return np.mean(per_sample_loss)\n",
    "\n",
    "x = np.array([5,-4,5,-6])\n",
    "y = np.array([1,0,1,0])\n",
    "\n",
    "bce_loss = BCE_logits_Loss(y,x)\n",
    "\n",
    "print(f\"bce损失为{bce_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7221bec",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22827680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bce损失为0.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def MSE_Loss(lables, y_predict):\n",
    "    per_sample_loss = (lables-y_predict)**2\n",
    "    return np.mean(per_sample_loss)\n",
    "\n",
    "x = np.array([1,2,3,4])\n",
    "y = np.array([1,2,4,4])\n",
    "\n",
    "mes_loss = MSE_Loss(y,x)\n",
    "\n",
    "print(f\"bce损失为{mes_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d9858",
   "metadata": {},
   "source": [
    "### focal_Loss\n",
    "\n",
    "\n",
    "当面临类别极度不平衡的数据时，标准的交叉熵损失会因大量易分样本的主导而失效。Focal Loss 通过引入动态调制因子，强制模型聚焦于训练过程中的“硬核”样本，是解决此类问题的关键技术。\n",
    "\n",
    "核心公式回顾\n",
    "\n",
    "Focal Loss 在标准交叉熵的基础上，增加了权重因子 $\\\\alpha$ 和调制因子 $(1-p\\_t)^\\gamma$。其统一形式简洁而强大：\n",
    "\n",
    "$$L_{\\text{focal}} = -\\alpha_t (1-p_t)^\\gamma \\log(p_t)$$\n",
    "\n",
    "其中：\n",
    "\n",
    "  * $p\\_t$ 定义为：当真实标签 $y=1$ 时，$p\\_t=p$；当 $y=0$ 时，$p\\_t=1-p$。这里的 $p$ 是模型预测为正类的概率。\n",
    "  * $\\\\alpha\\_t$ 是类别平衡参数，$\\\\gamma$ 是聚焦参数，用于抑制易分样本的损失贡献。\n",
    "\n",
    "类别平衡参数 α 用于调节正负类别的整体权重比例：当正样本稀少时应增大 α（如 0.5→0.75 甚至更高），提高正样本在总损失中的占比；若负样本更重要或误报代价更高，则减小 α。本质是对“类别频率不平衡”进行线性重加权。聚焦参数 γ 用于控制对易分样本的抑制强度：γ=0 时退化为普通交叉熵；γ 越大（常用 1~3，默认 2），对高置信度样本的损失衰减越强，模型越专注于难样本；但过大可能导致训练不稳定或收敛变慢。实践中通常先根据类别比例确定 α，再在 γ∈[1,3] 内调节，以验证集表现为准微调。\n",
    "\n",
    "该实现接收模型输出的概率 (经过 Sigmoid 激活后) 作为输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d36c7061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "focal_loss损失为0.11094425300887605\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def focal_loss_prob(y_pred_prob, y_true, alpha=0.25, gamma=2.0, reduction=\"mean\", eps=1e-9):\n",
    "    p = np.clip(y_pred_prob, eps, 1 - eps)\n",
    "\n",
    "    pt = y_true * p + (1 - y_true) * (1 - p)\n",
    "    alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "\n",
    "    loss = -alpha_t * (1 - pt) ** gamma * np.log(pt)\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        return np.mean(loss)\n",
    "    elif reduction == \"sum\":\n",
    "        return np.sum(loss)\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.9,0.1,0.9,0.7])\n",
    "y = np.array([1,0,1,0])\n",
    "focal_loss损失为 = focal_loss_prob(x,y)\n",
    "\n",
    "print(f\"focal_loss损失为{focal_loss损失为}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284137e4",
   "metadata": {},
   "source": [
    "## 手写MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d44681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    多层感知机（MLP）的完整实现\n",
    "    Args:\n",
    "        input_dim (int): 输入特征维度\n",
    "        hidden_dims (list): 隐藏层维度列表，每个元素代表对应隐藏层的神经元数量\n",
    "        output_dim (int): 输出维度\n",
    "        activation (str): 激活函数类型，支持 'relu', 'tanh', 'sigmoid'\n",
    "        dropout_rate (float): Dropout概率，用于正则化\n",
    "        batch_norm (bool): 是否使用批归一化\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, activation='relu', dropout_rate=0.0, batch_norm=False):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.activation_name = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        # 构建网络层\n",
    "        layers = []\n",
    "        layer_dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        \n",
    "        # 逐层构建网络\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            # 线性变换层\n",
    "            linear_layer = nn.Linear(layer_dims[i], layer_dims[i + 1])\n",
    "            layers.append(linear_layer)\n",
    "            \n",
    "            # 如果不是最后一层，添加激活函数、批归一化和dropout\n",
    "            if i < len(layer_dims) - 2:\n",
    "                # 批归一化\n",
    "                if self.batch_norm:\n",
    "                    layers.append(nn.LayerNorm(layer_dims[i + 1]))\n",
    "                \n",
    "                # 激活函数\n",
    "                if activation == 'relu':\n",
    "                    layers.append(nn.ReLU())\n",
    "                elif activation == 'tanh':\n",
    "                    layers.append(nn.Tanh())\n",
    "                elif activation == 'sigmoid':\n",
    "                    layers.append(nn.Sigmoid())\n",
    "                else:\n",
    "                    raise ValueError(f\"不支持的激活函数: {activation}\")\n",
    "                \n",
    "                # Dropout正则化\n",
    "                if dropout_rate > 0:\n",
    "                    layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # 将所有层组合成Sequential模块\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # 初始化网络参数\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        网络参数初始化\n",
    "        使用Xavier/Glorot初始化方法，这对于深层网络的训练非常重要\n",
    "        \"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Xavier均匀分布初始化\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                # 偏置项初始化为0\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dfec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# 常见顺序是：Linear -> LayerNorm -> Activation -> Dropout\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, activation='relu', dropout=0.1, layernorm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        layer_dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        layers = []\n",
    "\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            layers.append(nn.Linear(layer_dims[i], layer_dims[i+1]))\n",
    "\n",
    "            if i < len(layer_dims) - 2:  # 最后一层不要激活/Dropout/Norm\n",
    "                if layernorm:\n",
    "                    layers.append(nn.LayerNorm(layer_dims[i+1]))      # nn.LayerNorm 必须指定 特征维度大小（即输入最后一维的大小）\n",
    "\n",
    "                if activation == 'relu':\n",
    "                    layers.append(nn.ReLU())\n",
    "                elif activation == 'sigmoid':\n",
    "                    layers.append(nn.Sigmoid())\n",
    "                elif activation == 'tanh':\n",
    "                    layers.append(nn.Tanh())\n",
    "\n",
    "                # Dropout接收的输入是经过ReLU激活后的向量，然后对这些激活值进行随机屏蔽，直接影响的是传递给下一层的信息\n",
    "                if dropout > 0:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # *被称为\"解包操作符\"（unpacking operator），它的作用是将一个可迭代对象（如列表、元组）中的元素逐个取出，作为独立的参数传递给函数。\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.initialize_weight()\n",
    "\n",
    "    def initialize_weight(self):\n",
    "        for module in self.modules():       # 调用self.modules()会依次返回整个MLP模型、第一个Linear层、LayerNorm层、ReLU层、第二个Linear层等等\n",
    "            if isinstance(module, nn.Linear):       # Linear层有权重矩阵和偏置向量需要初始化\n",
    "                nn.init.xavier_uniform_(module.weight)  # PyTorch的设计体系中，函数名末尾的下划线表示这是一个\"就地操作\"（in-place operation），意思是它会直接修改传入的张量，而不是返回一个新的张量。\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "mlp = MLP(32, [128, 64, 32], 1)\n",
    "x = torch.randn(1, 2, 32)\n",
    "print(x)\n",
    "y = mlp(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a23e7",
   "metadata": {},
   "source": [
    "## 混合精度训练AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bb65d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1], step[0], loss:2.30487060546875\n",
      "epoch[1], step[10], loss:2.2474365234375\n",
      "epoch[1], step[20], loss:2.1429443359375\n",
      "epoch[1], step[30], loss:1.953338623046875\n",
      "epoch[1], step[40], loss:1.589691162109375\n",
      "epoch[1], step[50], loss:1.1607837677001953\n",
      "epoch[1], step[60], loss:0.8270969390869141\n",
      "epoch[1], step[70], loss:0.7493977546691895\n",
      "epoch[1], step[80], loss:0.6877131462097168\n",
      "epoch[1], step[90], loss:0.5060857534408569\n",
      "epoch[1], step[100], loss:0.49372947216033936\n",
      "epoch[1], step[110], loss:0.503061830997467\n",
      "epoch[1], step[120], loss:0.4905708432197571\n",
      "epoch[1], step[130], loss:0.32233864068984985\n",
      "epoch[1], step[140], loss:0.44921380281448364\n",
      "epoch[1], step[150], loss:0.46268683671951294\n",
      "epoch[1], step[160], loss:0.36531856656074524\n",
      "epoch[1], step[170], loss:0.29948681592941284\n",
      "epoch[1], step[180], loss:0.33051609992980957\n",
      "epoch[1], step[190], loss:0.34873807430267334\n",
      "epoch[1], step[200], loss:0.36760321259498596\n",
      "epoch[1], step[210], loss:0.31475210189819336\n",
      "epoch[1], step[220], loss:0.3854420781135559\n",
      "epoch[1], step[230], loss:0.36614060401916504\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 1.基础配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# 2.数据准备\n",
    "# transforms.ToTensor:()第一，数据类型转换(numpy或者image数据转为tensor)；第二，数值归一化。将0-255范围的像素值缩放到0-1范围；第三，维度重排[通道数，高度，宽度]\n",
    "# Compose将多个transform串联成一个处理链，数据依次通过每个环节。\n",
    "transform = transforms.Compose([transforms.ToTensor()]) \n",
    "train_dataset = datasets.MNIST(root = r'D:\\科研\\搜广推\\04_手撕算法题\\大模型_推荐算法_手撕题\\data', train=True, download=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "# 3.模型\n",
    "class simplemodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),  \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),  \n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 损失函数、优化器等设置\n",
    "\"\"\"\n",
    "需要移动到 GPU 的是参与大规模计算的张量数据，包括模型参数和输入数据\n",
    "损失函数和优化器是操作这些张量的“工具”，它们会自动在张量所在的设备上执行操作，所以不需要手动移动\n",
    "\"\"\"\n",
    "model = simplemodel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 4.训练设置\n",
    "\"\"\"\n",
    "训练模式\n",
    "Dropout 层会按照设定的概率 p 随机地将一部分神经元的输出置为零。这是一种有效的正则化手段，可以防止模型过拟合\n",
    "BN 层会计算当前批次数据的均值和方差，并用它们来归一化数据。同时，它还会维护一个全局的均值和方差，这个全局值是根据训练过程中所有批次的统计数据通过滑动平均更新的\n",
    "\n",
    "推理模式\n",
    "Dropout 层会“关闭”，不再随机丢弃任何神经元，而是让所有神经元的输出都通过。这样可以保证在预测时模型的输出是确定和稳定的\n",
    "BN 层会“冻结”，不再计算当前批次的均值和方差，而是直接使用在整个训练集上学习到的全局均值和方差来进行归一化。这保证了在推理时，即使输入只有一个样本，也能得到一致和稳定的结果\n",
    "\"\"\"\n",
    "\n",
    "model.train()\n",
    "def main():\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(\"cuda\", dtype=dtype):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()     # PyTorch 会计算出模型中每个参数的梯度,默认情况下，这些新计算出的梯度会加到该参数已有的梯度上（如果已有梯度存在的话），而不是覆盖它们\n",
    "        optimizer.step()    # 更新梯度\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"epoch[{1}], step[{batch_idx}], loss:{loss.item()}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
