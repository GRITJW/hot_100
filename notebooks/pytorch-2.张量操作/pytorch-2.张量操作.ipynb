{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# torch.tensor: 从 Python 列表或标量直接创建张量，数据会被深拷贝，可指定 dtype 和 device\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32)\n",
    "print(x)\n",
    "# tensor([[1., 2.],\n",
    "#         [3., 4.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float64)\n",
      "tensor([1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# torch.from_numpy: 从 NumPy 数组创建张量，与原数组共享内存（修改一方会影响另一方）\n",
    "arr = np.array([1.0, 2.0, 3.0])\n",
    "t1 = torch.from_numpy(arr)\n",
    "print(t1)           # tensor([1., 2., 3.], dtype=torch.float64)\n",
    "\n",
    "\n",
    "# torch.as_tensor: 与 from_numpy 类似，但更通用，支持任意可转换对象；若输入已是张量且 dtype/device 匹配则不拷贝\n",
    "t2 = torch.as_tensor(arr)\n",
    "print(t2)           # tensor([1., 2., 3.], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.zeros: 创建指定形状的全零张量\n",
    "print(torch.zeros(2, 3))\n",
    "# tensor([[0., 0., 0.],\n",
    "#         [0., 0., 0.]])\n",
    "\n",
    "\n",
    "# torch.ones: 创建指定形状的全一张量\n",
    "print(torch.ones(2, 3))\n",
    "# tensor([[1., 1., 1.],\n",
    "#         [1., 1., 1.]])\n",
    "\n",
    "\n",
    "# torch.eye: 创建二维单位矩阵（对角线为1，其余为0）\n",
    "print(torch.eye(3))\n",
    "# tensor([[1., 0., 0.],\n",
    "#         [0., 1., 0.],\n",
    "#         [0., 0., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.zeros_like: 创建与给定张量形状、dtype、device 完全相同的全零张量\n",
    "ref = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(torch.zeros_like(ref))\n",
    "# tensor([[0., 0.],\n",
    "#         [0., 0.]])\n",
    "\n",
    "\n",
    "# torch.ones_like: 创建与给定张量形状、dtype、device 完全相同的全一张量\n",
    "print(torch.ones_like(ref))\n",
    "# tensor([[1., 1.],\n",
    "#         [1., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.0963e+08, 1.1084e-42, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
      "tensor([1., 2., 3.], grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# torch.empty: 创建一个未初始化的张量（值为内存残留数据），仅分配内存，速度快，需后续手动赋值\n",
    "print(torch.empty(2, 3))\n",
    "# tensor([[...]])  值不确定\n",
    "\n",
    "\n",
    "# torch.clone: 深拷贝一个张量，保留梯度计算图（与 detach().copy_() 不同），常用于保存中间状态\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x.clone()\n",
    "print(y)            # tensor([1., 2., 3.], grad_fn=<CloneBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2371, 0.4549, 0.5133],\n",
      "        [0.2349, 0.9595, 0.6308]])\n",
      "tensor([[-0.2359, -1.0181, -1.8251],\n",
      "        [ 1.8605, -0.3005,  1.9893]])\n",
      "tensor([[2.6202, 5.5717, 5.1439],\n",
      "        [8.5762, 7.3996, 7.0859]])\n"
     ]
    }
   ],
   "source": [
    "# torch.rand: 创建填充了 [0, 1) 均匀分布随机数的张量\n",
    "print(torch.rand(2, 3))\n",
    "\n",
    "\n",
    "# torch.randn: 创建填充了标准正态分布（均值0，标准差1）随机数的张量\n",
    "print(torch.randn(2, 3))\n",
    "\n",
    "\n",
    "# torch.normal: 创建填充了指定均值和标准差的正态分布随机数的张量\n",
    "print(torch.normal(mean=5.0, std=2.0, size=(2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 9, 3, 5],\n",
      "        [0, 7, 3, 5]])\n"
     ]
    }
   ],
   "source": [
    "# torch.randint: 生成指定形状的整数张量，元素从区间 [low, high) 内均匀随机采样\n",
    "print(torch.randint(low=0, high=10, size=(2, 4)))\n",
    "# tensor([[3, 7, 1, 9],\n",
    "#         [0, 5, 4, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1400, 3.1400, 3.1400, 3.1400],\n",
      "        [3.1400, 3.1400, 3.1400, 3.1400]])\n"
     ]
    }
   ],
   "source": [
    "# torch.full: 生成指定形状的张量，所有元素初始化为同一个指定填充值\n",
    "print(torch.full((2, 4), fill_value=3.14))\n",
    "# tensor([[3.1400, 3.1400, 3.1400, 3.1400],\n",
    "#         [3.1400, 3.1400, 3.1400, 3.1400]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# torch.arange: 创建从 start 到 end（不含）按 step 步长递增的一维整数或浮点张量\n",
    "print(torch.arange(start=0, end=10, step=2))\n",
    "# tensor([0, 2, 4, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
      "tensor([   1.,   10.,  100., 1000.])\n"
     ]
    }
   ],
   "source": [
    "# torch.linspace: 在 [start, end] 区间内均匀分布生成指定数量的点，构成一维张量\n",
    "print(torch.linspace(0, 1, steps=5))\n",
    "# tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
    "\n",
    "\n",
    "# torch.logspace: 在 [10^start, 10^end] 区间内对数尺度均匀分布生成指定数量的点\n",
    "print(torch.logspace(start=0, end=3, steps=4))\n",
    "# tensor([   1.,   10.,  100., 1000.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 2, 0, 4, 3, 7, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "# torch.randperm: 生成 0 到 n-1 的随机排列整数张量，常用于数据集随机打乱（shuffle）\n",
    "print(torch.randperm(8))\n",
    "# tensor([5, 2, 7, 0, 3, 6, 1, 4])  每次结果不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "# torch.cat: 将多个张量在指定的已有维度上拼接（拼接后该维度大小为各张量之和，其余维度不变）\n",
    "a = torch.zeros(2, 3)\n",
    "b = torch.ones(2, 3)\n",
    "print(torch.cat([a, b], dim=0).shape)   # torch.Size([4, 3])  沿行拼接\n",
    "print(torch.cat([a, b], dim=1).shape)   # torch.Size([2, 6])  沿列拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.stack: 在新增维度上堆叠一组形状相同的张量，结果比输入多一个维度\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "print(torch.stack([a, b], dim=0))   # shape: (2, 3)，dim=0 处新增维度\n",
    "# tensor([[1., 2., 3.],\n",
    "#         [4., 5., 6.]])\n",
    "print(torch.stack([a, b], dim=1))   # shape: (3, 2)，dim=1 处新增维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "# tensor.view: 在不拷贝数据的情况下重塑张量形状，要求张量在内存中连续存储\n",
    "x = torch.arange(12.0)\n",
    "print(x.view(3, 4))         # shape: (3, 4)\n",
    "print(x.view(3, -1))        # -1 表示自动推断该维度大小，shape: (3, 4)\n",
    "\n",
    "\n",
    "# tensor.reshape: 功能与 view 相同，但允许非连续张量（内部会自动拷贝），更安全\n",
    "y = x.reshape(2, 6)\n",
    "print(y.shape)               # torch.Size([2, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1, 4])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# tensor.squeeze: 移除张量中所有大小为 1 的维度；可指定 dim 只移除特定维度\n",
    "x = torch.zeros(1, 3, 1, 4)\n",
    "print(x.squeeze().shape)            # torch.Size([3, 4])\n",
    "print(x.squeeze(dim=0).shape)       # torch.Size([3, 1, 4])  只去除 dim=0\n",
    "\n",
    "\n",
    "# tensor.unsqueeze: 在指定位置插入一个大小为 1 的新维度（常用于匹配批量维度）\n",
    "y = torch.tensor([1.0, 2.0, 3.0])  # shape: (3,)\n",
    "print(y.unsqueeze(0).shape)         # torch.Size([1, 3])  在最前插入\n",
    "print(y.unsqueeze(1).shape)         # torch.Size([3, 1])  在最后插入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3])\n",
      "torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# tensor.permute: 按指定顺序重新排列张量的所有维度（是 transpose 的推广，可一次交换多个维度）\n",
    "x = torch.randn(2, 3, 4)           # (batch, height, width)\n",
    "print(x.permute(0, 2, 1).shape)    # torch.Size([2, 4, 3])  交换 dim1 和 dim2\n",
    "print(x.permute(2, 0, 1).shape)    # torch.Size([4, 2, 3])  将 dim2 移至最前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.]])\n",
      "torch.Size([3, 4])\n",
      "tensor([[1, 2, 1, 2, 1, 2],\n",
      "        [3, 4, 3, 4, 3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [1, 2],\n",
      "        [3, 4]])\n",
      "torch.Size([4, 6])\n"
     ]
    }
   ],
   "source": [
    "# tensor.expand_as: 将张量沿大小为 1 的维度扩展到与目标张量相同的形状（不拷贝数据，共享内存）\n",
    "a = torch.tensor([[1.0], [2.0], [3.0]])   # shape: (3, 1)\n",
    "ref = torch.zeros(3, 4)\n",
    "print(a.expand_as(ref))\n",
    "print(a.expand_as(ref).shape)             # torch.Size([3, 4])\n",
    "\n",
    "\n",
    "# tensor.repeat: 沿每个维度将张量重复指定次数（真实拷贝数据，产生新张量）\n",
    "x = torch.tensor([[1, 2],[3, 4]])\n",
    "print(x.repeat(1, 3))                        # tensor([1, 2, 3, 1, 2, 3, 1, 2, 3])\n",
    "print(x.repeat(2, 1))               # torch.Size([2, 3])\n",
    "\n",
    "\n",
    "# torch.tile: 与 tensor.repeat 等价的函数式接口，NumPy 风格，沿各维度平铺张量\n",
    "print(torch.tile(x, dims=(2, 3)).shape)   # torch.Size([2, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 3., 0.])\n"
     ]
    }
   ],
   "source": [
    "# torch.where: 条件选择操作，逐元素地从 x 或 y 中选取值：condition 为 True 取 x，否则取 y\n",
    "x = torch.tensor([1.0, -2.0, 3.0, -4.0])\n",
    "y = torch.zeros_like(x)\n",
    "print(torch.where(x > 0, x, y))\n",
    "# tensor([1., 0., 3., 0.])  —— 相当于 ReLU 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(False)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# torch.any: 测试张量中是否至少有一个元素为非零（True），可指定 dim 沿某维度规约\n",
    "x = torch.tensor([0, 0, 1, 0])\n",
    "print(torch.any(x.bool()))          # tensor(True)\n",
    "print(torch.any(x.bool(), dim=0))   # tensor(True)\n",
    "\n",
    "\n",
    "# torch.all: 测试张量中所有元素是否均为非零（True），可指定 dim 沿某维度规约\n",
    "print(torch.all(x.bool()))          # tensor(False)  存在0元素\n",
    "y = torch.tensor([1, 2, 3])\n",
    "print(torch.all(y.bool()))          # tensor(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [1, 0],\n",
      "        [1, 2]])\n",
      "tensor([0, 1, 1]) tensor([1, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "# torch.nonzero: 返回输入张量中所有非零元素的索引，结果形状为 (N, ndim)（推荐用 as_tuple=True）\n",
    "x = torch.tensor([[0, 1, 0],\n",
    "                   [2, 0, 3]])\n",
    "print(torch.nonzero(x))\n",
    "# tensor([[0, 1],   <- 第0行第1列\n",
    "#         [1, 0],   <- 第1行第0列\n",
    "#         [1, 2]])  <- 第1行第2列\n",
    "\n",
    "# 等价写法（返回各维度索引的元组）\n",
    "rows, cols = torch.nonzero(x, as_tuple=True)\n",
    "print(rows, cols)   # tensor([0, 1, 1]) tensor([1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 1., 1.])\n",
      "tensor([2, 1, 3])\n",
      "tensor([-1.2629,  3.1870,  8.8229])\n",
      "tensor([2., 5., 8.])\n"
     ]
    }
   ],
   "source": [
    "# torch.bernoulli: 从伯努利分布中抽取样本，输入张量中每个元素作为该位置取 1 的概率\n",
    "p = torch.tensor([0.2, 0.5, 0.8, 0.9])\n",
    "print(torch.bernoulli(p))          # tensor([0., 1., 1., 1.])  随机结果\n",
    "\n",
    "\n",
    "# torch.multinomial: 从多项分布中按权重（不要求归一化）抽取指定数量的样本索引，可选有放回/无放回\n",
    "weights = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "print(torch.multinomial(weights, num_samples=3, replacement=False))\n",
    "# 高权重索引被选中概率更高，如 tensor([3, 2, 1])\n",
    "\n",
    "\n",
    "# torch.normal: 从指定均值和标准差的正态分布中抽取样本（均值和标准差可以是张量，逐元素独立采样）\n",
    "mean = torch.tensor([0.0, 5.0, 10.0])\n",
    "std  = torch.tensor([1.0, 2.0,  3.0])\n",
    "print(torch.normal(mean, std))\n",
    "\n",
    "\n",
    "# torch.poisson: 从泊松分布中抽取样本，输入张量的每个元素作为对应位置的 lambda（期望值）\n",
    "lam = torch.tensor([1.0, 5.0, 10.0])\n",
    "print(torch.poisson(lam))          # tensor([ 2.,  4., 11.])  随机结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
