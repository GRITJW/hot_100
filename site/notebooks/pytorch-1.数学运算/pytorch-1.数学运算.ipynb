{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数学运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n",
      "tensor([-3., -3., -3.])\n",
      "tensor([ 4., 10., 18.])\n",
      "tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.add: 对两个张量逐元素相加，支持广播机制，等价于 a + b\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "print(torch.add(a, b))          # tensor([5., 7., 9.])\n",
    "\n",
    "\n",
    "# torch.sub: 对两个张量逐元素相减，支持广播机制，等价于 a - b\n",
    "print(torch.sub(a, b))          # tensor([-3., -3., -3.])\n",
    "\n",
    "\n",
    "# torch.mul: 对两个张量逐元素相乘（Hadamard积），等价于 a * b\n",
    "print(torch.mul(a, b))          # tensor([ 4., 10., 18.])\n",
    "\n",
    "\n",
    "# torch.div: 对两个张量逐元素相除，等价于 a / b\n",
    "print(torch.div(a, b))          # tensor([0.2500, 0.4000, 0.5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4., 10., 18.])\n",
      "tensor(32.)\n"
     ]
    }
   ],
   "source": [
    "# torch.mul: 同上，逐元素相乘（此处与 torch.dot 对比）\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "print(torch.mul(a, b))          # tensor([ 4., 10., 18.])  —— 逐元素\n",
    "\n",
    "\n",
    "# torch.dot: 计算两个一维张量的点积（内积），返回标量\n",
    "print(torch.dot(a, b))          # tensor(32.)  (1*4 + 2*5 + 3*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.mm: 计算两个二维矩阵的矩阵乘法（不支持广播）\n",
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "B = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "print(torch.mm(A, B))\n",
    "# tensor([[19., 22.],\n",
    "#         [43., 50.]])\n",
    "\n",
    "\n",
    "# torch.matmul: 通用矩阵乘法，支持广播和批量维度，是 torch.mm 的超集\n",
    "print(torch.matmul(A, B))\n",
    "# tensor([[19., 22.],\n",
    "#         [43., 50.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  9., 16.])\n",
      "tensor([1.0000, 2.7183, 7.3891])\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# torch.pow: 对张量逐元素求幂，即 x^n\n",
    "x = torch.tensor([2.0, 3.0, 4.0])\n",
    "print(torch.pow(x, 2))          # tensor([ 4.,  9., 16.])\n",
    "\n",
    "\n",
    "# torch.exp: 对张量逐元素计算自然常数 e 的指数，即 e^x\n",
    "print(torch.exp(torch.tensor([0.0, 1.0, 2.0])))\n",
    "# tensor([1.0000, 2.7183, 7.3891])\n",
    "\n",
    "\n",
    "# torch.sqrt: 对张量逐元素计算平方根，即 √x\n",
    "print(torch.sqrt(torch.tensor([1.0, 4.0, 9.0])))\n",
    "# tensor([1., 2., 3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor([4., 6.])\n",
      "tensor(2.5000)\n",
      "tensor([1.5000, 3.5000])\n",
      "tensor(1.2910)\n"
     ]
    }
   ],
   "source": [
    "# torch.sum: 计算张量所有元素（或指定维度）的求和\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(torch.sum(x))             # tensor(10.)  全局求和\n",
    "print(torch.sum(x, dim=0))      # tensor([4., 6.])  按列求和\n",
    "\n",
    "\n",
    "# torch.mean: 计算张量所有元素（或指定维度）的均值\n",
    "print(torch.mean(x))            # tensor(2.5000)\n",
    "print(torch.mean(x, dim=1))     # tensor([1.5000, 3.5000])  按行均值\n",
    "\n",
    "\n",
    "# torch.std: 计算张量所有元素（或指定维度）的标准差（默认无偏估计）\n",
    "print(torch.std(x))             # tensor(1.2910)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "torch.return_types.max(\n",
      "values=tensor(5.),\n",
      "indices=tensor(4))\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# torch.max: 返回张量的最大值；若指定 dim，则同时返回最大值和对应索引\n",
    "x = torch.tensor([3.0, 1.0, 4.0, 1.0, 5.0])\n",
    "print(torch.max(x))             # tensor(5.)\n",
    "print(torch.max(x, dim=0))      # torch.return_types.max(values=tensor(5.), indices=tensor(4))\n",
    "\n",
    "\n",
    "# torch.min: 返回张量的最小值；若指定 dim，则同时返回最小值和对应索引\n",
    "print(torch.min(x))             # tensor(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  3.,  6., 10.])\n",
      "tensor([ 1.,  2.,  6., 24.])\n"
     ]
    }
   ],
   "source": [
    "# torch.cumsum: 沿指定维度计算张量元素的累积和（前缀和）\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "print(torch.cumsum(x, dim=0))   # tensor([ 1.,  3.,  6., 10.])\n",
    "\n",
    "\n",
    "# torch.cumprod: 沿指定维度计算张量元素的累积积（前缀积）\n",
    "print(torch.cumprod(x, dim=0))  # tensor([ 1.,  2.,  6., 24.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.5000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# torch.clamp: 将张量中每个元素限制在 [min, max] 区间内（低于 min 则置为 min，高于 max 则置为 max）\n",
    "x = torch.tensor([-2.0, 0.5, 1.5, 3.0])\n",
    "print(torch.clamp(x, min=0.0, max=1.0))\n",
    "# tensor([0.0000, 0.5000, 1.0000, 1.0000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.einsum: 使用爱因斯坦求和约定，以简洁的字符串表达式描述并计算任意多线性代数运算\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 5)\n",
    "\n",
    "# 等价于矩阵乘法 A @ B\n",
    "C = torch.einsum('ij,jk->ik', A, B)\n",
    "print(C.shape)                  # torch.Size([3, 5])\n",
    "\n",
    "# 逐元素相乘后求和（批量点积）\n",
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(3, 4)\n",
    "dot = torch.einsum('ij,ij->i', a, b)\n",
    "print(dot.shape)                # torch.Size([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# torch.bmm: 批量矩阵乘法，输入须为三维张量 (batch, n, m) x (batch, m, p)，不支持广播\n",
    "batch_A = torch.randn(10, 3, 4)\n",
    "batch_B = torch.randn(10, 4, 5)\n",
    "print(torch.bmm(batch_A, batch_B).shape)       # torch.Size([10, 3, 5])\n",
    "\n",
    "\n",
    "# torch.batch_matmul: 即 torch.matmul 处理批量场景，支持广播，是 torch.bmm 的广义版\n",
    "# （PyTorch 中通常直接用 torch.matmul 或 @ 运算符）\n",
    "print(torch.matmul(batch_A, batch_B).shape)    # torch.Size([10, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 3., 1., 0., 0.]])\n",
      "tensor([[0., 3., 3., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.scatter: 将源张量的值沿指定轴散布到目标张量的指定索引处（out-of-place 写入）\n",
    "src = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "index = torch.tensor([[2, 0, 1]])\n",
    "dst = torch.zeros(1, 5)\n",
    "dst.scatter_(1, index, src)\n",
    "print(dst)   # tensor([[2., 3., 1., 0., 0.]])\n",
    "\n",
    "\n",
    "# torch.scatter_add: 与 scatter 类似，但对重复索引处的值执行累加而非覆盖\n",
    "dst2 = torch.zeros(1, 5)\n",
    "index2 = torch.tensor([[2, 2, 1]])\n",
    "dst2.scatter_add_(1, index2, src)\n",
    "print(dst2)  # tensor([[0., 3., 3., 0., 0.]])  index=2 处累加了 1+2=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[30, 10],\n",
      "        [50, 60]])\n"
     ]
    }
   ],
   "source": [
    "# torch.gather: 根据索引张量，从输入张量沿指定维度取值（scatter 的逆操作）\n",
    "x = torch.tensor([[10, 20, 30],\n",
    "                   [40, 50, 60]])\n",
    "index = torch.tensor([[2, 0],\n",
    "                       [1, 2]])\n",
    "print(torch.gather(x, dim=1, index=index))\n",
    "# tensor([[30, 10],\n",
    "#         [50, 60]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# torch.index_select: 根据给定的一维索引张量，从输入张量的指定维度选取对应的切片\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])\n",
    "idx = torch.tensor([0, 2])          # 选取第 0 行和第 2 行\n",
    "print(torch.index_select(x, dim=0, index=idx))\n",
    "# tensor([[1, 2, 3],\n",
    "#         [7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3., 5.])\n"
     ]
    }
   ],
   "source": [
    "# torch.masked_select: 根据布尔掩码（BoolTensor）从输入张量中选取为 True 的元素，返回一维张量\n",
    "x = torch.tensor([1.0, -2.0, 3.0, -4.0, 5.0])\n",
    "mask = x > 0\n",
    "print(torch.masked_select(x, mask))\n",
    "# tensor([1., 3., 5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "tensor([3., 4., 5.])\n",
      "tensor([6., 7., 8.])\n",
      "tensor([9.])\n",
      "tensor([0., 1., 2., 3.])\n",
      "tensor([4., 5., 6., 7.])\n",
      "tensor([8., 9.])\n"
     ]
    }
   ],
   "source": [
    "# torch.split: 将张量沿指定维度按给定大小（或大小列表）分割为多个子张量\n",
    "x = torch.arange(10.0)\n",
    "chunks = torch.split(x, split_size_or_sections=3, dim=0)\n",
    "for c in chunks:\n",
    "    print(c)\n",
    "# tensor([0., 1., 2.])  tensor([3., 4., 5.])  tensor([6., 7., 8.])  tensor([9.])\n",
    "\n",
    "\n",
    "# torch.chunk: 将张量沿指定维度尽量均等地分割为 n 块（最后一块可能更小）\n",
    "chunks2 = torch.chunk(x, chunks=3, dim=0)\n",
    "for c in chunks2:\n",
    "    print(c)\n",
    "# tensor([0., 1., 2., 3.])  tensor([4., 5., 6., 7.])  tensor([8., 9.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 3., 4., 5., 9.])\n",
      "tensor([1, 3, 0, 2, 4, 5])\n",
      "tensor([9., 5., 4.])\n",
      "tensor([5, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "# torch.sort: 对张量沿指定维度排序，返回排序后的值张量和对应原始索引张量\n",
    "x = torch.tensor([3.0, 1.0, 4.0, 1.0, 5.0, 9.0])\n",
    "values, indices = torch.sort(x)\n",
    "print(values)    # tensor([1., 1., 3., 4., 5., 9.])\n",
    "print(indices)   # tensor([1, 3, 0, 2, 4, 5])\n",
    "\n",
    "\n",
    "# torch.topk: 返回张量在指定维度上最大（或最小）的 k 个元素及其索引，常用于推荐召回阶段取 Top-K\n",
    "values_k, indices_k = torch.topk(x, k=3)\n",
    "print(values_k)   # tensor([9., 5., 4.])\n",
    "print(indices_k)  # tensor([5, 4, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([1.+2.j, 3.+4.j])\n"
     ]
    }
   ],
   "source": [
    "# torch.view_as_real: 将复数张量视作实数张量，最后一维扩展为 2（分别存储实部和虚部）\n",
    "z = torch.tensor([1+2j, 3+4j])\n",
    "print(torch.view_as_real(z))\n",
    "# tensor([[1., 2.],\n",
    "#         [3., 4.]])\n",
    "\n",
    "\n",
    "# torch.view_as_complex: 将实数张量的最后一维（大小须为2）解释为复数的实部和虚部，转为复数张量\n",
    "r = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(torch.view_as_complex(r))\n",
    "# tensor([1.+2.j, 3.+4.j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.0000+0.j, 2.0000+0.j])\n",
      "tensor([5.1167, 1.9544])\n",
      "torch.Size([2, 2]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# torch.eig: （已废弃，推荐用 torch.linalg.eig）计算方阵的特征值和特征向量\n",
    "A = torch.tensor([[4.0, 1.0], [2.0, 3.0]])\n",
    "eigenvalues, eigenvectors = torch.linalg.eig(A)\n",
    "print(eigenvalues)   # tensor([5.+0.j, 2.+0.j])\n",
    "\n",
    "\n",
    "# torch.svd: （已废弃，推荐用 torch.linalg.svd）对矩阵进行奇异值分解，返回 U、S、V\n",
    "U, S, Vh = torch.linalg.svd(A)\n",
    "print(S)             # 奇异值\n",
    "\n",
    "\n",
    "# torch.qr: （已废弃，推荐用 torch.linalg.qr）对矩阵进行 QR 分解，返回正交矩阵 Q 和上三角矩阵 R\n",
    "Q, R = torch.linalg.qr(A)\n",
    "print(Q.shape, R.shape)  # torch.Size([2, 2]) torch.Size([2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0000,  1.0000],\n",
      "        [ 1.5000, -0.5000]])\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.inverse: 计算方阵的逆矩阵，要求矩阵可逆（推荐新版用 torch.linalg.inv）\n",
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "A_inv = torch.inverse(A)  # 或 torch.linalg.inv(A)\n",
    "print(A_inv)\n",
    "# tensor([[-2.0000,  1.0000],\n",
    "#         [ 1.5000, -0.5000]])\n",
    "print(torch.mm(A, A_inv))  # 验证：应接近单位矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# tensor.t: 对二维张量进行转置（仅适用于二维，等价于 tensor.T）\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(A.t())\n",
    "# tensor([[1, 4],\n",
    "#         [2, 5],\n",
    "#         [3, 6]])\n",
    "\n",
    "\n",
    "# tensor.transpose: 交换张量的任意两个指定维度，适用于任意维度的张量\n",
    "B = torch.randn(2, 3, 4)\n",
    "print(B.transpose(0, 2).shape)  # torch.Size([4, 3, 2])  交换 dim0 和 dim2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
