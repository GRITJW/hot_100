{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60cf0595",
   "metadata": {},
   "source": [
    "## 1.注意力机制\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87909a4",
   "metadata": {},
   "source": [
    "### MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac455cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代码运行成功！\n",
      "输出张量的形状: torch.Size([5, 10, 64])\n",
      "注意力权重的形状: torch.Size([5, 4, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math \n",
    "class multiheadattention(torch.nn.Module):\n",
    "    def __init__(self, d, h):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        self.k = d // h\n",
    "\n",
    "        self.wqkv = torch.nn.Linear(d, d*3)\n",
    "        self.wo = torch.nn.Linear(d, d)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        B, L, D = x.shape\n",
    "        qkv = self.wqkv(x)\n",
    "        q, k, v = torch.chunk(qkv, 3, -1)\n",
    "        # 可以用reshap代替transpose\n",
    "        q = q.view(B, L, self.h, self.k).transpose(1, 2)\n",
    "        k = k.view(B, L, self.h, self.k).transpose(1, 2)\n",
    "        v = v.view(B, L, self.h, self.k).transpose(1, 2)\n",
    "\n",
    "        attention_score =torch.matmul(q, k.transpose(-1,-2)) / math.sqrt(self.k)\n",
    "        if mask is not None:\n",
    "            attention_score = torch.masked_fill(attention_score, mask=mask, value=-1e9)\n",
    "        attention_weight = torch.softmax(attention_score, dim=-1)\n",
    "        context = torch.matmul(attention_weight, v).transpose(1, 2).contiguous().view(B, L, D)\n",
    "        output = self.wo(context)\n",
    "        return output, attention_weight\n",
    "    \n",
    "# --- 测试代码 ---\n",
    "batch_size = 5\n",
    "max_seq_len = 10\n",
    "d_model = 64\n",
    "head = 4\n",
    "\n",
    "x = torch.randn(batch_size, max_seq_len, d_model)\n",
    "\n",
    "attention_model = multiheadattention(d_model, head)\n",
    "output, attention = attention_model(x) \n",
    "\n",
    "print(\"代码运行成功！\")\n",
    "print(\"输出张量的形状:\", output.shape)\n",
    "print(\"注意力权重的形状:\", attention.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b6a2a",
   "metadata": {},
   "source": [
    "### MQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    与原 MultiHeadAttention 的差异：\n",
    "    - Q: 维持与 MHA 相同，shape -> [B, h, L, d_k]\n",
    "    - K/V: 仅产生 1 组共享头，shape -> [B, 1, L, d_k]\n",
    "    这样在推理时 KV cache 只需缓存 1 份（而非 h 份）。\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_head):\n",
    "        super(MultiQueryAttention, self).__init__()\n",
    "        assert d_model % num_head == 0, \"d_model 必须能被 num_head 整除\"\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.d_k = d_model // num_head\n",
    "\n",
    "        # 与原实现的区别：\n",
    "        #   - Q 仍然映射到 d_model（然后 reshape 成 h 个头）\n",
    "        #   - K/V 只映射到 d_k（单头维度），且各 1 份\n",
    "        self.wq = nn.Linear(d_model, d_model)      # 生成多头的 Q\n",
    "        self.wkv = nn.Linear(d_model, 2 * self.d_k)  # 生成共享的 K、V（仅 1 头的维度）\n",
    "\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        # 1) 计算 Q、K、V\n",
    "        q = self.wq(x)                       # [B, L, d_model]\n",
    "        kv = self.wkv(x)                     # [B, L, 2*d_k]\n",
    "        k, v = torch.chunk(kv, 2, dim=-1)    # [B, L, d_k], [B, L, d_k]\n",
    "\n",
    "        # 2) 形状整理\n",
    "        # Q: [B, L, h, d_k] -> [B, h, L, d_k]\n",
    "        q = q.view(B, L, self.num_head, self.d_k).transpose(1, 2)  # [B, h, L, d_k]\n",
    "\n",
    "        # 共享 K/V：加一个“伪 head 维”=1，方便广播到 h\n",
    "        # K/V: [B, L, d_k] -> [B, 1, L, d_k]\n",
    "        k = k.unsqueeze(1)  # [B, 1, L, d_k]\n",
    "        v = v.unsqueeze(1)  # [B, 1, L, d_k]\n",
    "\n",
    "        # 3) 注意力分数：Q 与共享 K\n",
    "        # scores: [B, h, L, L]，这里利用了 K 在 head 维度上的广播\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.d_k)  # [B, h, L, L]\n",
    "\n",
    "        if mask is not None:\n",
    "            # 要求 mask 能广播到 [B, h, L, L]\n",
    "            # 例如 mask 形状可为 [B, 1, 1, L]（causal/ padding），或 [B, 1, L, L]\n",
    "            scores = scores.masked_fill(mask, -1e9)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)         # [B, h, L, L]\n",
    "\n",
    "        # 4) 加权求和：与共享 V 相乘（同样通过广播）\n",
    "        context = torch.matmul(attn, v)              # [B, h, L, d_k]\n",
    "\n",
    "        # 5) 还原回 [B, L, d_model] 并输出\n",
    "        context = context.transpose(1, 2).contiguous()         # [B, L, h, d_k]\n",
    "        context = context.view(B, L, self.d_model)             # [B, L, d_model]\n",
    "        output = self.wo(context)                               # [B, L, d_model]\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "# --- 简单测试（与原 MHA 测试保持一致） ---\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 2\n",
    "    d_model = 10\n",
    "    head = 2\n",
    "    max_seq_len = 5\n",
    "    x = torch.randn(batch_size, max_seq_len, d_model)\n",
    "\n",
    "    attention_model = MultiQueryAttention(d_model, head)\n",
    "    output, attention = attention_model(x)\n",
    "\n",
    "    print(\"代码运行成功！（MQA）\")\n",
    "    print(\"输出张量的形状:\", output.shape)     # 期望: [B, L, d_model]\n",
    "    print(\"注意力权重的形状:\", attention.shape)  # 期望: [B, h, L, L]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ea8c8",
   "metadata": {},
   "source": [
    "### GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9fb0aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代码运行成功！（GQA）\n",
      "输出张量形状: torch.Size([2, 5, 12])\n",
      "注意力形状  : torch.Size([2, 6, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    GQA: 把 h 个 Query 头分成 g 组；组内共享 1 套 K/V\n",
    "    - num_head = h\n",
    "    - num_kv_head = g (1 < g <= h, 且 h % g == 0)\n",
    "    形状约定：\n",
    "      Q: [B, h, L, d_k]\n",
    "      K,V(分组): [B, g, L, d_k]\n",
    "    计算时把 Q reshape 成 [B, g, h_per_group, L, d_k]，\n",
    "    与同组的 K,V 做注意力；最后再还原回 [B, h, L, d_k]。\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_head, num_kv_head):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        assert d_model % num_head == 0, \"d_model 必须能被 num_head 整除\"\n",
    "        assert 1 <= num_kv_head <= num_head, \"num_kv_head 必须在 [1, num_head] 范围内\"\n",
    "        assert num_head % num_kv_head == 0, \"num_head 必须能被 num_kv_head 整除（每组等量分配）\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head            # h\n",
    "        self.num_kv_head = num_kv_head      # g\n",
    "        self.d_k = d_model // num_head\n",
    "        self.h_per_group = self.num_head // self.num_kv_head  # h/g\n",
    "\n",
    "        # Q 仍映射到 d_model（随后 reshape 为 h 个头）\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # K/V 只映射到 g * d_k（随后 reshape 为 g 个“KV 头”）\n",
    "        self.wkv = nn.Linear(d_model, 2 * self.num_kv_head * self.d_k)\n",
    "\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        # 1) 投影\n",
    "        # Q: [B, L, d_model]\n",
    "        # KV: [B, L, 2 * g * d_k] -> split -> [B, L, g * d_k] 各自\n",
    "        q = self.wq(x)\n",
    "        kv = self.wkv(x)\n",
    "        k, v = torch.chunk(kv, 2, dim=-1)\n",
    "\n",
    "        # 2) 形状整理\n",
    "        # Q -> [B, h, L, d_k]\n",
    "        q = q.view(B, L, self.num_head, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # K,V -> [B, g, L, d_k]\n",
    "        k = k.view(B, L, self.num_kv_head, self.d_k).transpose(1, 2)\n",
    "        v = v.view(B, L, self.num_kv_head, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 把 Q 分组： [B, h, L, d_k] -> [B, g, h_per_group, L, d_k]\n",
    "        qg = q.view(B, self.num_kv_head, self.h_per_group, L, self.d_k)\n",
    "\n",
    "        # 为了与组内 K,V 做 batched matmul，给 K,V 加一个组内头维度=1，方便广播\n",
    "        # Kg, Vg: [B, g, 1, L, d_k]\n",
    "        Kg = k.unsqueeze(2)\n",
    "        Vg = v.unsqueeze(2)\n",
    "\n",
    "        # 3) 组内注意力分数： [B, g, h_per_group, L, L]\n",
    "        # 等价于：scores_g[b,g,hg] = qg[b,g,hg] @ Kg[b,g,0]^T / sqrt(d_k)\n",
    "        scores_g = torch.matmul(qg, Kg.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            # 要求 mask 能广播到 [B, 1 或 g, 1 或 h_per_group, L, L] 或最终 [B, h, L, L]\n",
    "            # 最常见做法：提供 [B, 1, 1, L, L]（或 [B, 1, 1, 1, L] 的causal/pad组合）\n",
    "            scores_g = scores_g.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_g = torch.softmax(scores_g, dim=-1)               # [B, g, h_per_group, L, L]\n",
    "\n",
    "        # 4) 组内加权求和：context_g: [B, g, h_per_group, L, d_k]\n",
    "        context_g = torch.matmul(attn_g, Vg)\n",
    "\n",
    "        # 5) 还原回所有头：先合并 g 与 h_per_group -> h\n",
    "        context = context_g.reshape(B, self.num_head, L, self.d_k)  # [B, h, L, d_k]\n",
    "        context = context.transpose(1, 2).contiguous()              # [B, L, h, d_k]\n",
    "        context = context.view(B, L, self.d_model)                  # [B, L, d_model]\n",
    "        output = self.wo(context)\n",
    "\n",
    "        # 同样给出注意力权重（按头展平回 [B, h, L, L]，便于对齐可视化）\n",
    "        attn = attn_g.reshape(B, self.num_head, L, L)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "# --- 简单测试（与原 MHA 测试风格一致） ---\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 2\n",
    "    d_model = 12\n",
    "    num_head = 6     # h\n",
    "    num_kv_head = 3  # g（每组 2 个 Q 头）\n",
    "    max_seq_len = 5\n",
    "\n",
    "    x = torch.randn(batch_size, max_seq_len, d_model)\n",
    "    gqa = GroupedQueryAttention(d_model, num_head, num_kv_head)\n",
    "    out, attn = gqa(x)\n",
    "\n",
    "    print(\"代码运行成功！（GQA）\")\n",
    "    print(\"输出张量形状:\", out.shape)      # 期望: [B, L, d_model]\n",
    "    print(\"注意力形状  :\", attn.shape)     # 期望: [B, h, L, L]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18dcedd",
   "metadata": {},
   "source": [
    "## 2.AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f4d59",
   "metadata": {},
   "source": [
    "### AUC\n",
    "基于排序（Rank / Mann–Whitney U）的 AUC 计算方法。\n",
    "\n",
    "输入：\n",
    "- labels: List[int] 或 1D numpy array\n",
    "    样本真实标签，取值为 {0, 1}\n",
    "- scores: List[float] 或 1D numpy array\n",
    "    模型预测分数，分数越大表示越可能为正样本\n",
    "\n",
    "输出：\n",
    "- auc: float\n",
    "    AUC 值，取值范围 [0, 1]\n",
    "\n",
    "核心思想：\n",
    "1. 按预测分数从小到大排序\n",
    "2. 扫描排序后的样本序列\n",
    "3. 每遇到一个正样本，统计其前面已有多少负样本\n",
    "    这些负样本都被该正样本“正确地排在后面”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cdd950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "def auc_rank(labels, scores):\n",
    " \n",
    "    # 转为 numpy array，便于排序和向量化操作\n",
    "    labels = np.asarray(labels)\n",
    "    scores = np.asarray(scores)\n",
    "    print(f\"labels:{labels}\")\n",
    "    print(f\"scores:{scores}\")\n",
    "\n",
    " \n",
    "    # 获取按照 score 从小到大排序后的索引\n",
    "    order = np.argsort(scores)\n",
    "    print(f\"order:{order}\")\n",
    " \n",
    "    # 按排序后的顺序重排标签\n",
    "    labels_sorted = labels[order]\n",
    "    print(f\"labels_sorted:{labels_sorted}\")\n",
    " \n",
    " \n",
    "    # 正样本数量 |P|\n",
    "    n_pos = np.sum(labels_sorted == 1)\n",
    "    print(f\"n_pos:{n_pos}\")\n",
    " \n",
    "    # 负样本数量 |N|\n",
    "    n_neg = np.sum(labels_sorted == 0)\n",
    "    print(f\"n_neg:{n_neg}\")\n",
    "    # 已扫描到的负样本数量（前缀负样本计数）\n",
    "    neg_count = 0\n",
    " \n",
    "    # 排序正确的正负样本对数量\n",
    "    correct = 0.0\n",
    " \n",
    "    # 从低分到高分扫描\n",
    "    for l in labels_sorted:\n",
    "        if l == 1:\n",
    "            # 当前是正样本：\n",
    "            # 它前面的所有负样本都满足 score_neg < score_pos\n",
    "            correct += neg_count\n",
    "        else:\n",
    "            # 当前是负样本，增加负样本计数\n",
    "            neg_count += 1\n",
    " \n",
    "    # AUC = 排序正确的正负样本对 / 总正负样本对\n",
    "    return correct / (n_pos * n_neg)\n",
    "\n",
    "label = [0, 1, 0, 0, 1, 0, 0, 1]\n",
    "q = [0.1, 0.9, 0.2, 0.8, 1, 0.2, 0.3, 0.8]\n",
    "auc  = auc_rank(label, q)\n",
    "print(f\"auc:{auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6e291",
   "metadata": {},
   "source": [
    "### GAUC\n",
    "基于排序的 GAUC 计算方法(Group AUC)。\n",
    "\n",
    "GAUC 通过在每个用户内部分别计算 AUC,然后进行加权平均,\n",
    "从而消除用户间基线差异的影响,更准确地评估模型的用户内排序能力。\n",
    "\n",
    "输入:\n",
    "- user_ids: List[str] 或 1D numpy array\n",
    "    每个样本对应的用户标识\n",
    "- labels: List[int] 或 1D numpy array  \n",
    "    样本真实标签,取值为 {0, 1}\n",
    "- scores: List[float] 或 1D numpy array\n",
    "    模型预测分数,分数越大表示越可能为正样本\n",
    "- weight_type: str, 默认 'impression'\n",
    "    权重类型,可选值:\n",
    "    - 'impression': 按用户曝光次数加权(工业界常用)\n",
    "    - 'uniform': 等权重,每个用户权重为 1\n",
    "    \n",
    "输出:\n",
    "- gauc: float\n",
    "    加权后的 GAUC 值,取值范围 [0, 1]\n",
    "- user_auc_dict: dict\n",
    "    每个用户的 AUC 值字典,用于分析不同用户的排序质量\n",
    "    \n",
    "核心思想:\n",
    "1. 将样本按 user_id 分组\n",
    "2. 在每个用户内部,使用 Rank-based 方法计算 AUC\n",
    "3. 根据指定的权重类型对所有用户的 AUC 进行加权平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d30a6af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gauc:0.8636363636363636\n",
      "user_auc_dict:{1: 1.0, 2: 1.0, 3: 0.5}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    " \n",
    "def gauc_rank(user_ids, labels, scores, weight_type='impression'):\n",
    "    \n",
    "    # 转为 numpy array\n",
    "    user_ids = np.asarray(user_ids)\n",
    "    labels = np.asarray(labels)\n",
    "    scores = np.asarray(scores)\n",
    "    \n",
    "    # 按用户分组:构建 user_id -> 样本索引列表 的映射\n",
    "    user_sample_dict = defaultdict(list)\n",
    "    for idx, user_id in enumerate(user_ids):\n",
    "        user_sample_dict[user_id].append(idx)\n",
    "    \n",
    "    # 存储每个用户的 AUC 和权重\n",
    "    user_auc_dict = {}\n",
    "    total_weighted_auc = 0.0\n",
    "    total_weight = 0.0\n",
    "    \n",
    "    # 遍历每个用户,计算其 AUC\n",
    "    for user_id, sample_indices in user_sample_dict.items():\n",
    "        # 提取该用户的所有样本,user_id=1,sample_indices= [0, 1, 2, 3]\n",
    "        user_labels = labels[sample_indices]\n",
    "        user_scores = scores[sample_indices]\n",
    "        \n",
    "        # 计算该用户内的正负样本数\n",
    "        n_pos = np.sum(user_labels == 1)\n",
    "        n_neg = np.sum(user_labels == 0)\n",
    "        \n",
    "        # 如果该用户只有正样本或只有负样本,无法计算 AUC\n",
    "        # 跳过该用户(也可以选择赋值为 0.5)\n",
    "        if n_pos == 0 or n_neg == 0:\n",
    "            continue\n",
    "            \n",
    "        # 使用 Rank-based 方法计算该用户的 AUC\n",
    "        # 按 score 从小到大排序\n",
    "        order = np.argsort(user_scores)\n",
    "        sorted_labels = user_labels[order]\n",
    "        \n",
    "        # 统计排序正确的正负样本对数\n",
    "        neg_count = 0\n",
    "        correct_pairs = 0.0\n",
    "        \n",
    "        for label in sorted_labels:\n",
    "            if label == 1:\n",
    "                # 正样本:其前面的所有负样本都被正确排序\n",
    "                correct_pairs += neg_count\n",
    "            else:\n",
    "                # 负样本:计数增加\n",
    "                neg_count += 1\n",
    "        \n",
    "        # 该用户的 AUC\n",
    "        user_auc = correct_pairs / (n_pos * n_neg)\n",
    "        user_auc_dict[user_id] = user_auc\n",
    "        \n",
    "        # 确定该用户的权重\n",
    "        if weight_type == 'impression':\n",
    "            # 按曝光次数加权\n",
    "            weight = len(sample_indices)\n",
    "        elif weight_type == 'uniform':\n",
    "            # 等权重\n",
    "            weight = 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown weight_type: {weight_type}\")\n",
    "        \n",
    "        # 累加加权 AUC\n",
    "        total_weighted_auc += user_auc * weight\n",
    "        total_weight += weight\n",
    "    \n",
    "    # 计算 GAUC\n",
    "    if total_weight == 0:\n",
    "        return 0.5, user_auc_dict\n",
    "    \n",
    "    gauc = total_weighted_auc / total_weight\n",
    "    print(f\"gauc:{gauc}\")\n",
    "    print(f\"user_auc_dict:{user_auc_dict}\")\n",
    "    return gauc, user_auc_dict\n",
    "\n",
    "label = [0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0]\n",
    "q = [0.1, 0.9, 0.2, 0.8, 1, 0.2, 0.3, 0.9, 0.7, 0.9, 0.7]\n",
    "user_id = [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3]\n",
    "_, _ = gauc_rank(user_id, label, q, weight_type='impression')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a88cc9",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ef5df",
   "metadata": {},
   "source": [
    "### BCE二类交叉熵实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88560854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bce损失为0.008514077508444018\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "class BCE_Logits_Loss:\n",
    "    \"\"\"\n",
    "    一个数值稳定的、基于 Logits 的二元交叉熵损失函数实现。\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction = 'mean'):\n",
    "\n",
    "        if reduction not in ['mean', 'sum', 'none']:\n",
    "            raise ValueError(\"reduction 必须是 'mean', 'sum', 或 'none'\")\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def __call__(self, y_pred_logits: np.ndarray, y_true: np.ndarray):\n",
    "        x = y_pred_logits\n",
    "        y = y_true\n",
    " \n",
    "        # 直接套用数值稳定的BCE公式\n",
    "        per_sample_loss = np.maximum(x, 0) - x * y + np.log(1 + np.exp(-np.abs(x)))\n",
    " \n",
    "        if self.reduction == \"mean\":\n",
    "            return np.mean(per_sample_loss)\n",
    "        elif self.reduction == \"sum\":\n",
    "            return np.sum(per_sample_loss)\n",
    "        else: # self.reduction == 'none'\n",
    "            return per_sample_loss\n",
    "        \n",
    "x = np.array([5,-4,5,-6])\n",
    "y = np.array([1,0,1,0])\n",
    "\n",
    "mse_loss = BCE_Logits_Loss(reduction='mean')\n",
    "loss = mse_loss(x,y)\n",
    "\n",
    "print(f\"bce损失为{loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7221bec",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6788a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse损失为0.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "class MSE_Loss:\n",
    "    \"\"\"\n",
    "    一个数值稳定的、基于 Logits 的二元交叉熵损失函数实现。\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction = 'mean'):\n",
    "\n",
    "        if reduction not in ['mean', 'sum', 'none']:\n",
    "            raise ValueError(\"reduction 必须是 'mean', 'sum', 或 'none'\")\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def __call__(self, y_pred_logits: np.ndarray, y_true: np.ndarray):\n",
    "        \n",
    "        x = y_pred_logits\n",
    "        y = y_true\n",
    " \n",
    "        # 直接套用数值稳定的BCE公式\n",
    "        per_sample_loss = (y-x)**2\n",
    " \n",
    "        if self.reduction == \"mean\":\n",
    "            return np.mean(per_sample_loss)\n",
    "        elif self.reduction == \"sum\":\n",
    "            return np.sum(per_sample_loss)\n",
    "        else: # self.reduction == 'none'\n",
    "            return per_sample_loss\n",
    "        \n",
    "x = np.array([1,2,3,4])\n",
    "y = np.array([1,2,4,4])\n",
    "\n",
    "mse_loss = MSE_Loss(reduction='mean')\n",
    "loss = mse_loss(x,y)\n",
    "\n",
    "print(f\"mse损失为{loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d9858",
   "metadata": {},
   "source": [
    "### focal_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c7061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FocalLoss:\n",
    "    \"\"\"\n",
    "    一个简洁且高效的 Focal Loss 实现，用于处理类别不平衡问题。\n",
    "    \n",
    "    该实现接收模型输出的概率 (经过 Sigmoid 激活后) 作为输入。\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha: float = 0.25, gamma: float = 2.0, \n",
    "                 reduction = 'mean', eps: float = 1e-9):\n",
    "        \"\"\"\n",
    "        初始化 Focal Loss。\n",
    "\n",
    "        参数:\n",
    "            alpha (float): 平衡参数，用于调节正负样本的权重。论文推荐 0.25。\n",
    "            gamma (float): 聚焦参数，用于动态调整样本权重，使模型聚焦于难分样本。论文推荐 2.0。\n",
    "            reduction (str): 指定损失的聚合方式 ('mean', 'sum', 'none')。\n",
    "            eps (float): 一个极小的数值，用于避免 log(0) 导致的计算溢出。\n",
    "        \"\"\"\n",
    "        if reduction not in ['mean', 'sum', 'none']:\n",
    "            raise ValueError(\"reduction 必须是 'mean', 'sum', 或 'none'\")\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "    \n",
    "    def __call__(self, y_pred_prob: np.ndarray, y_true: np.ndarray) -> np.ndarray | float:\n",
    "\n",
    "        # 1. 裁剪概率值以保证数值稳定性\n",
    "        p = np.clip(y_pred_prob, self.eps, 1.0 - self.eps)\n",
    "\n",
    "        # 2. 计算正负样本的损失项，并用 y_true 和 (1-y_true) 作为掩码\n",
    "        # 正样本 (y_true=1) 的损失部分\n",
    "        loss_pos = -self.alpha * ((1 - p) ** self.gamma) * np.log(p) * y_true\n",
    "        \n",
    "        # 负样本 (y_true=0) 的损失部分\n",
    "        loss_neg = -(1 - self.alpha) * (p ** self.gamma) * np.log(1 - p) * (1 - y_true)\n",
    "\n",
    "        # 3. 将两部分损失相加得到每个样本的最终损失\n",
    "        per_sample_loss = loss_pos + loss_neg\n",
    "\n",
    "        # 4. 根据指定的 reduction 策略聚合损失\n",
    "        if self.reduction == \"mean\":\n",
    "            return np.mean(per_sample_loss)\n",
    "        elif self.reduction == \"sum\":\n",
    "            return np.sum(per_sample_loss)\n",
    "        else: # self.reduction == 'none'\n",
    "            return per_sample_loss\n",
    "\n",
    "x = np.array([0.9,0.1,0.9,0.7])\n",
    "y = np.array([1,0,1,0])\n",
    "\n",
    "mse_loss = FocalLoss(reduction='mean')\n",
    "loss = mse_loss(x,y)\n",
    "\n",
    "print(f\"focal_bce损失为{loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284137e4",
   "metadata": {},
   "source": [
    "## 手写MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d44681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    多层感知机（MLP）的完整实现\n",
    "    Args:\n",
    "        input_dim (int): 输入特征维度\n",
    "        hidden_dims (list): 隐藏层维度列表，每个元素代表对应隐藏层的神经元数量\n",
    "        output_dim (int): 输出维度\n",
    "        activation (str): 激活函数类型，支持 'relu', 'tanh', 'sigmoid'\n",
    "        dropout_rate (float): Dropout概率，用于正则化\n",
    "        batch_norm (bool): 是否使用批归一化\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, \n",
    "                 activation='relu', dropout_rate=0.0, batch_norm=False):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.activation_name = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        # 构建网络层\n",
    "        layers = []\n",
    "        layer_dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        \n",
    "        # 逐层构建网络\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            # 线性变换层\n",
    "            linear_layer = nn.Linear(layer_dims[i], layer_dims[i + 1])\n",
    "            layers.append(linear_layer)\n",
    "            \n",
    "            # 如果不是最后一层，添加激活函数、批归一化和dropout\n",
    "            if i < len(layer_dims) - 2:\n",
    "                # 批归一化（在激活函数之前）\n",
    "                if self.batch_norm:\n",
    "                    layers.append(nn.BatchNorm1d(layer_dims[i + 1]))\n",
    "                \n",
    "                # 激活函数\n",
    "                if activation == 'relu':\n",
    "                    layers.append(nn.ReLU())\n",
    "                elif activation == 'tanh':\n",
    "                    layers.append(nn.Tanh())\n",
    "                elif activation == 'sigmoid':\n",
    "                    layers.append(nn.Sigmoid())\n",
    "                else:\n",
    "                    raise ValueError(f\"不支持的激活函数: {activation}\")\n",
    "                \n",
    "                # Dropout正则化\n",
    "                if dropout_rate > 0:\n",
    "                    layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # 将所有层组合成Sequential模块\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # 初始化网络参数\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        网络参数初始化\n",
    "        使用Xavier/Glorot初始化方法，这对于深层网络的训练非常重要\n",
    "        \"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Xavier均匀分布初始化\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                # 偏置项初始化为0\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播过程\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): 输入张量，形状为 (batch_size, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: 输出张量，形状为 (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96dfec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.0681,  1.5619,  0.0909, -0.9272, -0.8474, -1.1844,  0.3646,\n",
      "          -0.8629, -1.4969,  0.1424, -0.8461,  0.8752, -0.1210,  1.0228,\n",
      "          -0.0918, -1.0681,  2.0352,  0.7658,  0.7031,  0.6318,  1.5409,\n",
      "          -0.9169,  0.3927, -1.3012,  0.5303, -1.2943, -1.3301, -0.8466,\n",
      "           2.1570,  0.6033,  0.2530,  1.4443],\n",
      "         [ 0.4359,  0.5136,  1.7633, -1.3619,  0.7331,  0.5869, -0.7370,\n",
      "          -0.1460,  0.8217,  0.5016, -0.6113,  1.1776,  0.4734,  1.2617,\n",
      "          -0.3058,  0.2861, -1.4580, -0.0126, -0.3054,  1.1936,  1.0763,\n",
      "           1.4527,  0.2317,  0.1980, -0.0380, -0.1888,  0.3183, -0.8052,\n",
      "          -1.8534,  0.8622, -0.7915, -1.5688]]])\n",
      "tensor([[[-0.8437],\n",
      "         [-0.1871]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# 常见顺序是：Linear -> LayerNorm -> Activation -> Dropout\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, activation='relu', dropout=0.1, layernorm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        layer_dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        layers = []\n",
    "\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            layers.append(nn.Linear(layer_dims[i], layer_dims[i+1]))\n",
    "\n",
    "            if i < len(layer_dims) - 2:  # 最后一层不要激活/Dropout/Norm\n",
    "                if layernorm:\n",
    "                    layers.append(nn.LayerNorm(layer_dims[i+1]))      # nn.LayerNorm 必须指定 特征维度大小（即输入最后一维的大小）\n",
    "\n",
    "                if activation == 'relu':\n",
    "                    layers.append(nn.ReLU())\n",
    "                elif activation == 'sigmoid':\n",
    "                    layers.append(nn.Sigmoid())\n",
    "                elif activation == 'tanh':\n",
    "                    layers.append(nn.Tanh())\n",
    "\n",
    "                # Dropout接收的输入是经过ReLU激活后的向量，然后对这些激活值进行随机屏蔽，直接影响的是传递给下一层的信息\n",
    "                if dropout > 0:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # *被称为\"解包操作符\"（unpacking operator），它的作用是将一个可迭代对象（如列表、元组）中的元素逐个取出，作为独立的参数传递给函数。\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.initialize_weight()\n",
    "\n",
    "    def initialize_weight(self):\n",
    "        for module in self.modules():       # 调用self.modules()会依次返回整个MLP模型、第一个Linear层、LayerNorm层、ReLU层、第二个Linear层等等\n",
    "            if isinstance(module, nn.Linear):       # Linear层有权重矩阵和偏置向量需要初始化\n",
    "                nn.init.xavier_uniform_(module.weight)  # PyTorch的设计体系中，函数名末尾的下划线表示这是一个\"就地操作\"（in-place operation），意思是它会直接修改传入的张量，而不是返回一个新的张量。\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "mlp = MLP(32, [128, 64, 32], 1)\n",
    "x = torch.randn(1, 2, 32)\n",
    "print(x)\n",
    "y = mlp(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a23e7",
   "metadata": {},
   "source": [
    "## 混合精度训练AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bb65d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1], step[0], loss:2.30328369140625\n",
      "epoch[1], step[10], loss:2.25201416015625\n",
      "epoch[1], step[20], loss:2.155517578125\n",
      "epoch[1], step[30], loss:1.97998046875\n",
      "epoch[1], step[40], loss:1.7359619140625\n",
      "epoch[1], step[50], loss:1.3389091491699219\n",
      "epoch[1], step[60], loss:0.8904438018798828\n",
      "epoch[1], step[70], loss:0.7717809677124023\n",
      "epoch[1], step[80], loss:0.7068207263946533\n",
      "epoch[1], step[90], loss:0.5415992736816406\n",
      "epoch[1], step[100], loss:0.4719715714454651\n",
      "epoch[1], step[110], loss:0.47473612427711487\n",
      "epoch[1], step[120], loss:0.37358441948890686\n",
      "epoch[1], step[130], loss:0.4280766248703003\n",
      "epoch[1], step[140], loss:0.42194801568984985\n",
      "epoch[1], step[150], loss:0.3845781683921814\n",
      "epoch[1], step[160], loss:0.29847246408462524\n",
      "epoch[1], step[170], loss:0.41989171504974365\n",
      "epoch[1], step[180], loss:0.36476024985313416\n",
      "epoch[1], step[190], loss:0.32573196291923523\n",
      "epoch[1], step[200], loss:0.3932267427444458\n",
      "epoch[1], step[210], loss:0.3183577060699463\n",
      "epoch[1], step[220], loss:0.2995343804359436\n",
      "epoch[1], step[230], loss:0.293863445520401\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 1.基础配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# 2.数据准备\n",
    "# transforms.ToTensor:()第一，数据类型转换(numpy或者image数据转为tensor)；第二，数值归一化。将0-255范围的像素值缩放到0-1范围；第三，维度重排[通道数，高度，宽度]\n",
    "# Compose将多个transform串联成一个处理链，数据依次通过每个环节。\n",
    "transform = transforms.Compose([transforms.ToTensor()]) \n",
    "train_dataset = datasets.MNIST(root = r'D:\\科研\\搜广推\\04_手撕算法题\\大模型_推荐算法_手撕题\\data', train=True, download=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "# 3.模型\n",
    "class simplemodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),  \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),  \n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 损失函数、优化器等设置\n",
    "\"\"\"\n",
    "需要移动到 GPU 的是参与大规模计算的张量数据，包括模型参数和输入数据\n",
    "损失函数和优化器是操作这些张量的“工具”，它们会自动在张量所在的设备上执行操作，所以不需要手动移动\n",
    "\"\"\"\n",
    "model = simplemodel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 4.训练设置\n",
    "\"\"\"\n",
    "训练模式\n",
    "Dropout 层会按照设定的概率 p 随机地将一部分神经元的输出置为零。这是一种有效的正则化手段，可以防止模型过拟合\n",
    "BN 层会计算当前批次数据的均值和方差，并用它们来归一化数据。同时，它还会维护一个全局的均值和方差，这个全局值是根据训练过程中所有批次的统计数据通过滑动平均更新的\n",
    "\n",
    "推理模式\n",
    "Dropout 层会“关闭”，不再随机丢弃任何神经元，而是让所有神经元的输出都通过。这样可以保证在预测时模型的输出是确定和稳定的\n",
    "BN 层会“冻结”，不再计算当前批次的均值和方差，而是直接使用在整个训练集上学习到的全局均值和方差来进行归一化。这保证了在推理时，即使输入只有一个样本，也能得到一致和稳定的结果\n",
    "\"\"\"\n",
    "\n",
    "model.train()\n",
    "def main():\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(\"cuda\", dtype=dtype):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()     # PyTorch 会计算出模型中每个参数的梯度,默认情况下，这些新计算出的梯度会加到该参数已有的梯度上（如果已有梯度存在的话），而不是覆盖它们\n",
    "        optimizer.step()    # 更新梯度\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"epoch[{1}], step[{batch_idx}], loss:{loss.item()}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
